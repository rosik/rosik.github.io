## 1. Тезисы

------

Tarantool – это база данных и сервер приложений в одном флаконе. Даже
для небольших проектов такого флакона часто бывает недостаточно.

В презентации я расскажу о новых возможностях Tarantool, которые
упрощают создание распределенных систем, и как мы к этому пришли:

* Как объединить несколько разнородных приложений и заставить их
  работать сообща.
* Почему нам нравится, что кластер управляет своей топологией
  самостоятельно.
* О принятии решений на основе внутреннего мониторинга.
* Проблемы производительности и их решение на кластере из 100+ узлов.

------

Readme in Tarantool repository states that it’s an in-memory database
and application server. But a single server is never enough even for
small projects.

In my presentation I’ll share experience of:

* combining several application servers in a cluster
* learning cluster to orchestrate itself
* making decisions based on internal monitoring
* And, of course, solving performance issues on large (100+) clusters.

------


## 2. Фактура

Тарантул это база данных и апп сервер
мы на его основе делаем коммерческие проекты

Современный мир живёт по аджайлу.

70 человек

Мы выложили в опенсорс новый фреймворк - картридж.

Свой доклад я собираюсь посвятить этомуи челенджам.

Для меня аспекты настройки и конфигурирования распределенной системы интереснее, чем ее функционирование.

Архтектура сервиса, которую мы должны были разработать - ну чисто микросервисная.
Первое что приходит на ум - докер.
Было 4 типа сервисов, мы могли бы сделать 4 докер имаджа и деплоить их по-отдельности.

Но с такой архитектурой очень неудобно жить разработчику.
Интеграционное тестирование такой архитектуры - это docker compose внутри докера.
Если делегировать оркестрацию докеру, то мы рискуем потерять половину данных, и даже не заметим.
Какой бы оркестратор мы не выбрали, он не будет знать ничего о внутреннем строении тарантула.
И мониторинга это тоже касается.

Мы решили написать свой оркестратор, заточенный исключительно под вшард.

Это было не сложно - у нас же есть сервер приложений.
Но тут внезапно возникла проблема курицы и яйца.
Никогда б не подул, что это может быть так увлекательно.
Проблема следующая - у нас есть один запущенный процесс тарантула без конфигурации,
и есть оркестратор, который должен этим процессом дирижировать.
На самом деле это может быть один и тот же процесс, мы же хотим,
чтобы тарантул сам собой кправлял.
И вот оркестратор должен как-то дать новому серверу понять,
что ему надо инициализировать базу. Но как, если этот сервер не знает на каком порту ему слушать,
с кем реплизироваться и ждёт команды от оркестратора. Получается, эту информацию
надо передавать ещё при запуске. Но тогда оркестратору будет нечем оркестрировать,
коль всё и так уже известно.
Задача не решается. Давайте немного отвлечёмся и попробуем переключиться на что-нибудь другое.

Я напомню, помимо оркестрации мы ещё хотели иметь какой-то умный мониторинг.
Так как у нас полное равноправие, то каждый член кластера будет мониторить всех окружающих.
Чтобы не нагружать сеть алгоритмической сложностью N^2 ради мониторинга существует
семейство протоколов называемых gossip.

Расказ про госсип.

Война с апи edit_topology


---------------

которые встретились нам на пути. Я буду делать упор на те вещи,
которые менее специфичны для тарантула чтобы всем было интересно.


Я в аннотации выписал ряд тем, и первая из них - как объединить несколько разнородных приложений.

Несколько разнородных приложений - это по сути микросервисная архитектура.
Тема достаточно попутярная, но у неё есть недостатки, от которых очень хотелось уйти,
и в итоге получилось.

Второй топик - самостоятельное управление топологией является логическим приложением первого.
Реализовать это было не сложно, но некоторые архитектурные нюансы нам доставили дискомфорт,
когда мы стали пытаться развернуть кластер на 100 узлов. Я нарисую картинки,
обозначу эти проблемные места.

Третий пункт аннотации про принятие решений тоже перекликается с первыми двумя,
но это скорее история провала, нежели успеха. Оказалось,

Ну и на-последок - про настоящий хайлоад. Мы взяли планку в 500 инстансов тарантула
в одном кластере и изучаем его стабильность со всех сторон.

-- Придумать пример масштабирования бизнес логики


### Schedule

1. Я, мы, чем занимаемся
1.
1.
1. Проблема - нужно масштабироваться
1. Вшард существует давно. Как он работает. (идея архитектуры)
1. Всех проблем не решает
1.
1.
1. Сложная оркестрация. Человеческие ошибки + история с прошлого хайлоада
1. Остается - масштабирование бизнес-логики. + история

Нам нужна возможность легко и просто масштабироваться.
И при этом не лишить себя возможности компактно запускать тесты.

1.
1.
1.
1.
1.
1.
1.
1.
1.
1.

1.
1.
1.
1.
1.
1.
1.
1.
1.
1.

1.
1.
1.
1.
1.
1.
1.
1.
1.
1.